{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This code is a copy, original code was run on top of a Databricks cluster, python 3.10.12\n",
    "\n",
    "Using the HuggingFace model at: [tiiuae/falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets==2.16.0\n",
    "!pip install -q trl\n",
    "!pip install -q bitsandbytes\n",
    "!pip install -q git+https://github.com/huggingface/transformers\n",
    "!pip install -q peft\n",
    "!pip install -q --upgrade accelerate\n",
    "#==0.27.2\n",
    "!pip install -q --upgrade torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, GenerationConfig)\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = compute_dtype,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"tiiuae/falcon-7b-instruct\",\n",
    "        quantization_config = bnb_config,\n",
    "        device_map = {\"\": 0},\n",
    "        trust_remote_code = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0.1,\n",
    "    r = 64,\n",
    "    bias = \"none\",\n",
    "    task_type = \"CAUSAL_LM\",\n",
    "    target_modules = [\n",
    "        \"query_key_value\"\n",
    "        # Other possible layers\n",
    "        # \"dense\",\n",
    "        # \"dense_h_to_4h\",\n",
    "        # \"dense_4h_to_h\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b-instruct\", trust_remote_code = True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "dir_path = '/dbfs/mnt/boidspoc/raw/research_stuff/LoRA/Tiiuae-falcon-7b-instruct-LoRA-2/'\n",
    "model_name_is = f\"peft-dialogue-summary-training-{str(int(time.time()))}___test\"\n",
    "output_dir = f'{dir_path}/{model_name_is}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    optim = 'paged_adamw_32bit',\n",
    "    save_steps = 500, #250\n",
    "    fp16 = True,\n",
    "    logging_steps = 100,\n",
    "    learning_rate = 2e-4,\n",
    "    max_grad_norm = 0.3,\n",
    "    max_steps = 1000, # 10000\n",
    "    warmup_ratio = 0.03,\n",
    "    lr_scheduler_type = \"constant\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False\n",
    "dataset = load_dataset(\"timdettmers/openassistant-guanaco\", split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = dataset,\n",
    "    peft_config = peft_config,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 512,\n",
    "    tokenizer = tokenizer,\n",
    "    args = training_arguments,\n",
    "    packing = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in trainer.model.named_modules():\n",
    "    if \"norm\" in name:\n",
    "        module = module.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train() # Took 16h, loss started at 1.9 and finished at 0.8+-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f\"{output_dir}/output_dir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = compute_dtype,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"tiiuae/falcon-7b-instruct\", quantization_config = bnb_config, device_map = \"auto\", trust_remote_code = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(model, f\"{output_dir}/output_dir\", local_files_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = AutoTokenizer.from_pretrained('tiiuae/falcon-7b-instruct')\n",
    "tok.pad_token = tok.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a 4chan style greentext about someone who loves the new romantic comedy movie, with an ironic twist that re-contextualizes the story at the end of it. It should start with '>be me\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_encoding = tok(prompt, return_tensors = \"pt\").to(\"cuda:0\")\n",
    "peft_outputs = model.generate(\n",
    "  input_ids = peft_encoding.input_ids, \n",
    "  generation_config = GenerationConfig(\n",
    "    max_new_tokens = 256, \n",
    "    pad_token_id = tok.eos_token_id, \n",
    "    eos_token_id = tok.eos_token_id, \n",
    "    attention_mask = peft_encoding.attention_mask, \n",
    "    temperature = 0.1, \n",
    "    top_p = 0.1, \n",
    "    repetition_penalty = 1.2, \n",
    "    num_return_sequences = 1,\n",
    "  )\n",
    ")\n",
    "peft_text_output = tok.decode(peft_outputs[0], skip_special_tokens = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing `peft_text_output` resulted with the following final:\n",
    "\n",
    "    '>be me ### Assistant: Hi there!\\n\\nI recently watched the new romantic comedy movie on 4chan. I must say, I was pleasantly surprised by the plot twists and turns. I especially enjoyed the way the story played with my expectations and made me question my assumptions.\\n\\nThe story centered around a guy named John who had just broken up with his ex-girlfriend. The movie followed John as he navigated his post-breakup blues and tried to find himself again. Along the way, he met a woman named Sarah, who swept him off his feet.\\n\\nAt first glance, it seemed like a typical romcom scenario. But as the story progressed, things started to get interesting. John discovered that Sarah was actually an escort working out of Las Vegas. This revelation completely changed the game for him and opened up a whole new world of possibilities.\\n\\nJohn quickly realized that he could make a lot of money by exploiting this situation. He started advertising himself as an escort on social media and began charging clients for dates and companionship. Before long, he was making more money than he ever had before and living a life of luxury.\\n\\nBut then something unexpected happened.'\n",
    "\n",
    "For sure it could be better, but this is just an example in order to figure out how to use the Peft library with LoRA/QLoRA"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
